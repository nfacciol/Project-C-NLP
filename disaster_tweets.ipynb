{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facciola NLP Disaster Tweet Model\n\n- In this competition we are building an NLP model to predict whether a Tweet is about a real disaster or not. ","metadata":{}},{"cell_type":"code","source":"import warnings\nimport os\nimport pandas as pd\nimport numpy as np\n\n\nwarnings.filterwarnings('ignore')\nDATA_DIR = \"/kaggle/input/nlp-getting-started/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:48.045049Z","iopub.execute_input":"2024-07-29T20:06:48.045867Z","iopub.status.idle":"2024-07-29T20:06:48.413562Z","shell.execute_reply.started":"2024-07-29T20:06:48.045831Z","shell.execute_reply":"2024-07-29T20:06:48.412802Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Import the training data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:48.414945Z","iopub.execute_input":"2024-07-29T20:06:48.415282Z","iopub.status.idle":"2024-07-29T20:06:48.479469Z","shell.execute_reply.started":"2024-07-29T20:06:48.415259Z","shell.execute_reply":"2024-07-29T20:06:48.478576Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Import the test data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:48.480726Z","iopub.execute_input":"2024-07-29T20:06:48.481423Z","iopub.status.idle":"2024-07-29T20:06:48.509750Z","shell.execute_reply.started":"2024-07-29T20:06:48.481390Z","shell.execute_reply":"2024-07-29T20:06:48.508905Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## EDA\n- examine the structure of the data","metadata":{}},{"cell_type":"code","source":"print(\"Train set info\")\nprint(train_df.info())\nprint()\nprint(\"Test set info\")\nprint(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:48.512107Z","iopub.execute_input":"2024-07-29T20:06:48.512811Z","iopub.status.idle":"2024-07-29T20:06:48.538872Z","shell.execute_reply.started":"2024-07-29T20:06:48.512779Z","shell.execute_reply":"2024-07-29T20:06:48.538005Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Train set info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\nNone\n\nTest set info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 102.1+ KB\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Cleaning\n- here we clean the text data by removing unneccssary characters, handling missing values, and normalizing text","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.set_proxy('http://proxy-dmz.intel.com:911/')\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n#print(stop_words)\n\ndef clean_text(text):\n   #remove URLS\n   text = re.sub(r'http\\S+', '', text)\n   #remove HTML tags\n   text = re.sub(r'<.*?>', '', text)\n   # Remove non-alphanumeric characters except hashtags and mentions\n   text = re.sub(r'[^a-zA-Z0-9\\s#@]', '', text)\n   # Convert to lowercase\n   text = text.lower()\n   # Remove stopwords\n   text = ' '.join([word for word in text.split() if word not in stop_words])\n   return text\n\ntrain_df['clean_text'] = train_df['text'].apply(clean_text)\ntest_df['clean_text'] = test_df['text'].apply(clean_text)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:48.539962Z","iopub.execute_input":"2024-07-29T20:06:48.540227Z","iopub.status.idle":"2024-07-29T20:06:49.845722Z","shell.execute_reply.started":"2024-07-29T20:06:48.540204Z","shell.execute_reply":"2024-07-29T20:06:49.844828Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n[nltk_data]     service not known>\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target                                         clean_text  \n0       1      deeds reason #earthquake may allah forgive us  \n1       1              forest fire near la ronge sask canada  \n2       1  residents asked shelter place notified officer...  \n3       1  13000 people receive #wildfires evacuation ord...  \n4       1  got sent photo ruby #alaska smoke #wildfires p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>deeds reason #earthquake may allah forgive us</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>forest fire near la ronge sask canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>residents asked shelter place notified officer...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>13000 people receive #wildfires evacuation ord...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Feature Engineering\n- **text length**: Calculate the length of each tweet. This can help capture information about tweet complexity or verbosity.\n- **word count**: Count the number of words in each tweet, which may provide insight into tweet structure.\n- **hashtag count**: Count the number of hashtags in each tweet, as this can be indicative of topic relevance or trending discussions.\n- **mention count**: Count the number of user mentions, which can indicate the tweet's engagement level.\n- **hasUrl**: Create a binary feature indicating whether the tweet contains a URL.\n- **sentiment score**: Use a pre-trained sentiment analyzer to get a sentiment score for each tweet.\n- **pos tags**: Count the occurrence of different parts of speech in each tweet.\n- **profanity count**: Count the number of profane words in each tweet using a predefined list of profane words.","metadata":{}},{"cell_type":"code","source":"train_df['text_length'] = train_df['text'].apply(len)\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:49.846936Z","iopub.execute_input":"2024-07-29T20:06:49.847562Z","iopub.status.idle":"2024-07-29T20:06:49.867679Z","shell.execute_reply.started":"2024-07-29T20:06:49.847530Z","shell.execute_reply":"2024-07-29T20:06:49.866699Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"         id       keyword                 location  \\\n3905   5555     flattened       Pomfret/Providence   \n7204  10320        weapon  www.twitch.tv/PKSparkxx   \n5634   8035      refugees                      NaN   \n6736   9653  thunderstorm     Killafornia made me    \n4899   6974      massacre                  Ecuador   \n\n                                                   text  target  \\\n3905  'the fallacy is it is up to the steam roller. ...       0   \n7204  Slosher is GOAT. Freaking love that weapon. Ca...       0   \n5634  ...//..// whao.. 12000 Nigerian refugees repat...       1   \n6736  9:35 pm. Thunderstorm. No rain. 90 degrees. Th...       1   \n4899  Don't mess with my Daddy I can be a massacre. ...       0   \n\n                                             clean_text  text_length  \n3905  fallacy steam roller object whether flattened ...          139  \n7204  slosher goat freaking love weapon cant wait ep...          130  \n5634  whao 12000 nigerian refugees repatriated cameroon           89  \n6736  935 pm thunderstorm rain 90 degrees weather weird           63  \n4899           dont mess daddy massacre #becarefulharry           61  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3905</th>\n      <td>5555</td>\n      <td>flattened</td>\n      <td>Pomfret/Providence</td>\n      <td>'the fallacy is it is up to the steam roller. ...</td>\n      <td>0</td>\n      <td>fallacy steam roller object whether flattened ...</td>\n      <td>139</td>\n    </tr>\n    <tr>\n      <th>7204</th>\n      <td>10320</td>\n      <td>weapon</td>\n      <td>www.twitch.tv/PKSparkxx</td>\n      <td>Slosher is GOAT. Freaking love that weapon. Ca...</td>\n      <td>0</td>\n      <td>slosher goat freaking love weapon cant wait ep...</td>\n      <td>130</td>\n    </tr>\n    <tr>\n      <th>5634</th>\n      <td>8035</td>\n      <td>refugees</td>\n      <td>NaN</td>\n      <td>...//..// whao.. 12000 Nigerian refugees repat...</td>\n      <td>1</td>\n      <td>whao 12000 nigerian refugees repatriated cameroon</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>6736</th>\n      <td>9653</td>\n      <td>thunderstorm</td>\n      <td>Killafornia made me</td>\n      <td>9:35 pm. Thunderstorm. No rain. 90 degrees. Th...</td>\n      <td>1</td>\n      <td>935 pm thunderstorm rain 90 degrees weather weird</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>4899</th>\n      <td>6974</td>\n      <td>massacre</td>\n      <td>Ecuador</td>\n      <td>Don't mess with my Daddy I can be a massacre. ...</td>\n      <td>0</td>\n      <td>dont mess daddy massacre #becarefulharry</td>\n      <td>61</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:49.868804Z","iopub.execute_input":"2024-07-29T20:06:49.869107Z","iopub.status.idle":"2024-07-29T20:06:49.896317Z","shell.execute_reply.started":"2024-07-29T20:06:49.869082Z","shell.execute_reply":"2024-07-29T20:06:49.895453Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         id              keyword      location  \\\n5823   8314               rubble        London   \n7212  10331               weapon  New York, NY   \n4297   6103             hellfire           NaN   \n140     201  airplane%20accident           NaN   \n1913   2753               curfew           NaN   \n\n                                                   text  target  \\\n5823  #360WiseNews : China's Stock Market Crash: Are...       1   \n7212  03/08/11: Police stop a 41-year-old in the Bro...       1   \n4297  @HellFire_eV @JackPERU1 then I do this to one ...       0   \n140   @AlexAllTimeLow awwww they're on an airplane a...       1   \n1913  @TheComedyQuote @50ShadezOfGrey the thirst has...       0   \n\n                                             clean_text  text_length  \\\n5823  #360wisenews chinas stock market crash gems ru...           95   \n7212  030811 police stop 41yearold bronx citing casi...          106   \n4297                         @hellfireev @jackperu1 one           58   \n140   @alexalltimelow awwww theyre airplane accident...          104   \n1913  @thecomedyquote @50shadezofgrey thirst curfew ...           79   \n\n      word_count  \n5823          13  \n7212          18  \n4297          11  \n140           17  \n1913           9  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5823</th>\n      <td>8314</td>\n      <td>rubble</td>\n      <td>London</td>\n      <td>#360WiseNews : China's Stock Market Crash: Are...</td>\n      <td>1</td>\n      <td>#360wisenews chinas stock market crash gems ru...</td>\n      <td>95</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>7212</th>\n      <td>10331</td>\n      <td>weapon</td>\n      <td>New York, NY</td>\n      <td>03/08/11: Police stop a 41-year-old in the Bro...</td>\n      <td>1</td>\n      <td>030811 police stop 41yearold bronx citing casi...</td>\n      <td>106</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>4297</th>\n      <td>6103</td>\n      <td>hellfire</td>\n      <td>NaN</td>\n      <td>@HellFire_eV @JackPERU1 then I do this to one ...</td>\n      <td>0</td>\n      <td>@hellfireev @jackperu1 one</td>\n      <td>58</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>201</td>\n      <td>airplane%20accident</td>\n      <td>NaN</td>\n      <td>@AlexAllTimeLow awwww they're on an airplane a...</td>\n      <td>1</td>\n      <td>@alexalltimelow awwww theyre airplane accident...</td>\n      <td>104</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1913</th>\n      <td>2753</td>\n      <td>curfew</td>\n      <td>NaN</td>\n      <td>@TheComedyQuote @50ShadezOfGrey the thirst has...</td>\n      <td>0</td>\n      <td>@thecomedyquote @50shadezofgrey thirst curfew ...</td>\n      <td>79</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['hashtag_count'] = train_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('#')]))\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:49.897803Z","iopub.execute_input":"2024-07-29T20:06:49.898163Z","iopub.status.idle":"2024-07-29T20:06:49.945044Z","shell.execute_reply.started":"2024-07-29T20:06:49.898129Z","shell.execute_reply":"2024-07-29T20:06:49.944232Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"         id   keyword     location  \\\n6695   9592   thunder          NaN   \n2276   3264  demolish          NaN   \n3599   5139     fatal          NaN   \n2425   3485  derailed  SEC Country   \n7502  10731     wreck    Canada BC   \n\n                                                   text  target  \\\n6695      That was the l9udest thunder I've  ever heard       0   \n2276     Ugh So hungry I'm going to demolish this food!       0   \n3599  @spookyfob @feelslikefob I am okay thank you y...       0   \n2425  @BobbyofHomewood @JOXRoundtable as in dropping...       0   \n7502      @raineishida lol...Im just a nervous wreck :P       0   \n\n                                             clean_text  text_length  \\\n6695                     l9udest thunder ive ever heard           45   \n2276                  ugh hungry im going demolish food           46   \n3599  @spookyfob @feelslikefob okay thank yes kindne...          118   \n2425  @bobbyofhomewood @joxroundtable dropping nospo...          115   \n7502                 @raineishida lolim nervous wreck p           45   \n\n      word_count  hashtag_count  \n6695           8              0  \n2276           9              0  \n3599          19              0  \n2425          17              0  \n7502           7              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6695</th>\n      <td>9592</td>\n      <td>thunder</td>\n      <td>NaN</td>\n      <td>That was the l9udest thunder I've  ever heard</td>\n      <td>0</td>\n      <td>l9udest thunder ive ever heard</td>\n      <td>45</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2276</th>\n      <td>3264</td>\n      <td>demolish</td>\n      <td>NaN</td>\n      <td>Ugh So hungry I'm going to demolish this food!</td>\n      <td>0</td>\n      <td>ugh hungry im going demolish food</td>\n      <td>46</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3599</th>\n      <td>5139</td>\n      <td>fatal</td>\n      <td>NaN</td>\n      <td>@spookyfob @feelslikefob I am okay thank you y...</td>\n      <td>0</td>\n      <td>@spookyfob @feelslikefob okay thank yes kindne...</td>\n      <td>118</td>\n      <td>19</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2425</th>\n      <td>3485</td>\n      <td>derailed</td>\n      <td>SEC Country</td>\n      <td>@BobbyofHomewood @JOXRoundtable as in dropping...</td>\n      <td>0</td>\n      <td>@bobbyofhomewood @joxroundtable dropping nospo...</td>\n      <td>115</td>\n      <td>17</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7502</th>\n      <td>10731</td>\n      <td>wreck</td>\n      <td>Canada BC</td>\n      <td>@raineishida lol...Im just a nervous wreck :P</td>\n      <td>0</td>\n      <td>@raineishida lolim nervous wreck p</td>\n      <td>45</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['mention_count'] = train_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('@')]))\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:49.946092Z","iopub.execute_input":"2024-07-29T20:06:49.946440Z","iopub.status.idle":"2024-07-29T20:06:49.992211Z","shell.execute_reply.started":"2024-07-29T20:06:49.946405Z","shell.execute_reply":"2024-07-29T20:06:49.991401Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         id       keyword location  \\\n3109   4463  electrocuted      USA   \n7607  10867           NaN      NaN   \n6986  10018       twister      NaN   \n1640   2368     collapsed   Paris    \n3080   4421   electrocute     Mass   \n\n                                                   text  target  \\\n3109  South Side factory where worker electrocuted p...       1   \n7607  #stormchase Violent Record Breaking EF-5 El Re...       1   \n6986            Twister was fun https://t.co/qCT6fb8wOn       0   \n1640  ... The pain of those seconds must have been a...       1   \n3080  @Mmchale13 *tries to electrocute self with pho...       0   \n\n                                             clean_text  text_length  \\\n3109  south side factory worker electrocuted pays 17...           99   \n7607  #stormchase violent record breaking ef5 el ren...          134   \n6986                                        twister fun           39   \n1640  pain seconds must awful heart burst lungs coll...          121   \n3080       @mmchale13 tries electrocute self phone cord           54   \n\n      word_count  hashtag_count  mention_count  \n3109          12              1              0  \n7607          16              1              0  \n6986           4              0              0  \n1640          24              0              0  \n3080           8              0              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3109</th>\n      <td>4463</td>\n      <td>electrocuted</td>\n      <td>USA</td>\n      <td>South Side factory where worker electrocuted p...</td>\n      <td>1</td>\n      <td>south side factory worker electrocuted pays 17...</td>\n      <td>99</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7607</th>\n      <td>10867</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n      <td>1</td>\n      <td>#stormchase violent record breaking ef5 el ren...</td>\n      <td>134</td>\n      <td>16</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6986</th>\n      <td>10018</td>\n      <td>twister</td>\n      <td>NaN</td>\n      <td>Twister was fun https://t.co/qCT6fb8wOn</td>\n      <td>0</td>\n      <td>twister fun</td>\n      <td>39</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1640</th>\n      <td>2368</td>\n      <td>collapsed</td>\n      <td>Paris</td>\n      <td>... The pain of those seconds must have been a...</td>\n      <td>1</td>\n      <td>pain seconds must awful heart burst lungs coll...</td>\n      <td>121</td>\n      <td>24</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3080</th>\n      <td>4421</td>\n      <td>electrocute</td>\n      <td>Mass</td>\n      <td>@Mmchale13 *tries to electrocute self with pho...</td>\n      <td>0</td>\n      <td>@mmchale13 tries electrocute self phone cord</td>\n      <td>54</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['has_url'] = train_df['text'].apply(lambda x: 1 if re.search(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", x) else 0)\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:49.995838Z","iopub.execute_input":"2024-07-29T20:06:49.996141Z","iopub.status.idle":"2024-07-29T20:06:50.035327Z","shell.execute_reply.started":"2024-07-29T20:06:49.996119Z","shell.execute_reply":"2024-07-29T20:06:50.034465Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         id       keyword     location  \\\n1972   2838       cyclone          NaN   \n865    1250         blood          NaN   \n7308  10458  wild%20fires  nap central   \n3109   4463  electrocuted          USA   \n5028   7169      mudslide   the burrow   \n\n                                                   text  target  \\\n1972  'I'm a cyclone passion overblown' https://t.co...       0   \n865   Bruh white people buy the ugliest shoes and th...       0   \n7308     Wild fires freak me the fuck out. Like hell no       1   \n3109  South Side factory where worker electrocuted p...       1   \n5028  DORETTE THATS THE NAME OF THE MUDSLIDE CAKE MAKER       0   \n\n                                             clean_text  text_length  \\\n1972                       im cyclone passion overblown           57   \n865   bruh white people buy ugliest shoes super tigh...           94   \n7308                    wild fires freak fuck like hell           46   \n3109  south side factory worker electrocuted pays 17...           99   \n5028             dorette thats name mudslide cake maker           49   \n\n      word_count  hashtag_count  mention_count  has_url  \n1972           6              0              0        1  \n865           18              0              0        0  \n7308          10              0              0        0  \n3109          12              1              0        1  \n5028           9              0              0        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>has_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1972</th>\n      <td>2838</td>\n      <td>cyclone</td>\n      <td>NaN</td>\n      <td>'I'm a cyclone passion overblown' https://t.co...</td>\n      <td>0</td>\n      <td>im cyclone passion overblown</td>\n      <td>57</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>865</th>\n      <td>1250</td>\n      <td>blood</td>\n      <td>NaN</td>\n      <td>Bruh white people buy the ugliest shoes and th...</td>\n      <td>0</td>\n      <td>bruh white people buy ugliest shoes super tigh...</td>\n      <td>94</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7308</th>\n      <td>10458</td>\n      <td>wild%20fires</td>\n      <td>nap central</td>\n      <td>Wild fires freak me the fuck out. Like hell no</td>\n      <td>1</td>\n      <td>wild fires freak fuck like hell</td>\n      <td>46</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3109</th>\n      <td>4463</td>\n      <td>electrocuted</td>\n      <td>USA</td>\n      <td>South Side factory where worker electrocuted p...</td>\n      <td>1</td>\n      <td>south side factory worker electrocuted pays 17...</td>\n      <td>99</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5028</th>\n      <td>7169</td>\n      <td>mudslide</td>\n      <td>the burrow</td>\n      <td>DORETTE THATS THE NAME OF THE MUDSLIDE CAKE MAKER</td>\n      <td>0</td>\n      <td>dorette thats name mudslide cake maker</td>\n      <td>49</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from textblob import TextBlob\ntrain_df['sentiment_score'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:50.036655Z","iopub.execute_input":"2024-07-29T20:06:50.037016Z","iopub.status.idle":"2024-07-29T20:06:52.075239Z","shell.execute_reply.started":"2024-07-29T20:06:50.036986Z","shell.execute_reply":"2024-07-29T20:06:52.074353Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"        id    keyword         location  \\\n5285  7552   outbreak        Indonesia   \n1837  2642    crashed          too far   \n6278  8969      storm       New Delhi    \n2265  3245    deluged  Los Angeles, CA   \n4409  6268  hijacking              NaN   \n\n                                                   text  target  \\\n5285  More than 40 families affected by the fatal ou...       1   \n1837  He was only .4 of a second faster than me and ...       0   \n6278       @johngreen storm and silence by @RobThier_EN       0   \n2265  @valdes1978 forgive me if I was a bit testy. H...       0   \n4409  @USAgov Koreans are performing hijacking of th...       1   \n\n                                             clean_text  text_length  \\\n5285  40 families affected fatal outbreak legionnair...          136   \n1837  4 second faster overtook twice crashed tru luv...          101   \n6278               @johngreen storm silence @robthieren           44   \n2265  @valdes1978 forgive bit testy deluged hatred a...          100   \n4409  @usagov koreans performing hijacking tokyo oly...           91   \n\n      word_count  hashtag_count  mention_count  has_url  sentiment_score  \n5285          20              0              0        1              0.5  \n1837          21              0              0        0              0.0  \n6278           6              0              2        0              0.0  \n2265          18              0              1        0              0.0  \n4409          10              0              1        1              0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>has_url</th>\n      <th>sentiment_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5285</th>\n      <td>7552</td>\n      <td>outbreak</td>\n      <td>Indonesia</td>\n      <td>More than 40 families affected by the fatal ou...</td>\n      <td>1</td>\n      <td>40 families affected fatal outbreak legionnair...</td>\n      <td>136</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1837</th>\n      <td>2642</td>\n      <td>crashed</td>\n      <td>too far</td>\n      <td>He was only .4 of a second faster than me and ...</td>\n      <td>0</td>\n      <td>4 second faster overtook twice crashed tru luv...</td>\n      <td>101</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6278</th>\n      <td>8969</td>\n      <td>storm</td>\n      <td>New Delhi</td>\n      <td>@johngreen storm and silence by @RobThier_EN</td>\n      <td>0</td>\n      <td>@johngreen storm silence @robthieren</td>\n      <td>44</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2265</th>\n      <td>3245</td>\n      <td>deluged</td>\n      <td>Los Angeles, CA</td>\n      <td>@valdes1978 forgive me if I was a bit testy. H...</td>\n      <td>0</td>\n      <td>@valdes1978 forgive bit testy deluged hatred a...</td>\n      <td>100</td>\n      <td>18</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4409</th>\n      <td>6268</td>\n      <td>hijacking</td>\n      <td>NaN</td>\n      <td>@USAgov Koreans are performing hijacking of th...</td>\n      <td>1</td>\n      <td>@usagov koreans performing hijacking tokyo oly...</td>\n      <td>91</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\ntrain_df['noun_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'NOUN' or token.pos_ == 'PROPN']))\ntrain_df['verb_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'VERB']))\ntrain_df['adverb_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADV']))\ntrain_df['adjective_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADJ']))\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:06:52.076311Z","iopub.execute_input":"2024-07-29T20:06:52.076607Z","iopub.status.idle":"2024-07-29T20:10:46.384539Z","shell.execute_reply.started":"2024-07-29T20:06:52.076583Z","shell.execute_reply":"2024-07-29T20:10:46.383624Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         id       keyword    location  \\\n1480   2133   catastrophe  Denver, CO   \n2843   4088     displaced         NaN   \n7319  10478  wild%20fires     Indiana   \n3619   5166    fatalities         NaN   \n3038   4359    earthquake       Earth   \n\n                                                   text  target  \\\n1480  #Denver CO #Insurance #Job: Claims Property Fi...       0   \n2843  PennLive - Two families displaced by Mechanics...       1   \n7319  'Your love will surely come find us\\nLike blaz...       0   \n3619  Las Vegas in top 5 cities for red-light runnin...       0   \n3038  1.9 earthquake occurred 15km E of Anchorage Al...       1   \n\n                                             clean_text  text_length  \\\n1480  #denver co #insurance #job claims property fie...          136   \n2843  pennlive two families displaced mechanicsburg ...          113   \n7319  love surely come find us like blazing wild fir...           78   \n3619  las vegas top 5 cities redlight running fatali...           91   \n3038  19 earthquake occurred 15km e anchorage alaska...          110   \n\n      word_count  hashtag_count  mention_count  has_url  sentiment_score  \\\n1480          17              3              0        1         0.000000   \n2843          17              0              0        1         0.000000   \n7319          14              0              0        0         0.366667   \n3619          13              0              0        1         0.500000   \n3038          14              2              0        1         0.000000   \n\n      noun_count  verb_count  adverb_count  adjective_count  \n1480          17           0             0                0  \n2843           7           2             0                0  \n7319           3           4             1                1  \n3619           7           1             0                2  \n3038           8           1             0                0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>has_url</th>\n      <th>sentiment_score</th>\n      <th>noun_count</th>\n      <th>verb_count</th>\n      <th>adverb_count</th>\n      <th>adjective_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1480</th>\n      <td>2133</td>\n      <td>catastrophe</td>\n      <td>Denver, CO</td>\n      <td>#Denver CO #Insurance #Job: Claims Property Fi...</td>\n      <td>0</td>\n      <td>#denver co #insurance #job claims property fie...</td>\n      <td>136</td>\n      <td>17</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2843</th>\n      <td>4088</td>\n      <td>displaced</td>\n      <td>NaN</td>\n      <td>PennLive - Two families displaced by Mechanics...</td>\n      <td>1</td>\n      <td>pennlive two families displaced mechanicsburg ...</td>\n      <td>113</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7319</th>\n      <td>10478</td>\n      <td>wild%20fires</td>\n      <td>Indiana</td>\n      <td>'Your love will surely come find us\\nLike blaz...</td>\n      <td>0</td>\n      <td>love surely come find us like blazing wild fir...</td>\n      <td>78</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.366667</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3619</th>\n      <td>5166</td>\n      <td>fatalities</td>\n      <td>NaN</td>\n      <td>Las Vegas in top 5 cities for red-light runnin...</td>\n      <td>0</td>\n      <td>las vegas top 5 cities redlight running fatali...</td>\n      <td>91</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.500000</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3038</th>\n      <td>4359</td>\n      <td>earthquake</td>\n      <td>Earth</td>\n      <td>1.9 earthquake occurred 15km E of Anchorage Al...</td>\n      <td>1</td>\n      <td>19 earthquake occurred 15km e anchorage alaska...</td>\n      <td>110</td>\n      <td>14</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install better_profanity\n\nfrom better_profanity import profanity\n\ntrain_df['profanity_count'] = train_df['text'].apply(lambda x: len([w for w in x if w in profanity.CENSOR_WORDSET]))\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:10:46.386067Z","iopub.execute_input":"2024-07-29T20:10:46.387185Z","iopub.status.idle":"2024-07-29T20:15:43.491655Z","shell.execute_reply.started":"2024-07-29T20:10:46.387147Z","shell.execute_reply":"2024-07-29T20:15:43.490373Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting better_profanity\n  Downloading better_profanity-0.7.0-py3-none-any.whl.metadata (7.1 kB)\nDownloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m737.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: better_profanity\nSuccessfully installed better_profanity-0.7.0\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         id keyword              location  \\\n7489  10711   wreck     Primum non nocere   \n562     812  battle                 Earth   \n3912   5563   flood         United States   \n1792   2571   crash  Melbourne, Australia   \n5741   8195    riot              Belgrade   \n\n                                                   text  target  \\\n7489  @GeorgeFoster72 and The Wreck of the Edmund Fi...       1   \n562   Check out this item I just got! [Phantasmal Cu...       0   \n3912  JKL cancels Flash Flood Warning for Bell Harla...       1   \n1792  #INCIDENT\\nCrash in Pascoe Vale South outbound...       1   \n5741  To All The Meat-Loving Feminists Of The World ...       0   \n\n                                             clean_text  text_length  \\\n7489            @georgefoster72 wreck edmund fitzgerald           54   \n562      check item got phantasmal cummerbund #warcraft           88   \n3912  jkl cancels flash flood warning bell harlan kn...           85   \n1792  #incident crash pascoe vale south outbound tul...          136   \n5741  meatloving feminists world riot grill arrived ...          135   \n\n      word_count  hashtag_count  mention_count  has_url  sentiment_score  \\\n7489           8              0              1        0              0.0   \n562           11              1              0        1              0.0   \n3912          12              1              0        1              0.0   \n1792          21              1              0        0              0.1   \n5741          21              1              0        1              0.0   \n\n      noun_count  verb_count  adverb_count  adjective_count  profanity_count  \n7489           3           0             0                0                0  \n562            5           2             1                0                0  \n3912           8           2             0                0                0  \n1792          15           0             0                0                0  \n5741          10           3             0                0                0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>has_url</th>\n      <th>sentiment_score</th>\n      <th>noun_count</th>\n      <th>verb_count</th>\n      <th>adverb_count</th>\n      <th>adjective_count</th>\n      <th>profanity_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7489</th>\n      <td>10711</td>\n      <td>wreck</td>\n      <td>Primum non nocere</td>\n      <td>@GeorgeFoster72 and The Wreck of the Edmund Fi...</td>\n      <td>1</td>\n      <td>@georgefoster72 wreck edmund fitzgerald</td>\n      <td>54</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>562</th>\n      <td>812</td>\n      <td>battle</td>\n      <td>Earth</td>\n      <td>Check out this item I just got! [Phantasmal Cu...</td>\n      <td>0</td>\n      <td>check item got phantasmal cummerbund #warcraft</td>\n      <td>88</td>\n      <td>11</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3912</th>\n      <td>5563</td>\n      <td>flood</td>\n      <td>United States</td>\n      <td>JKL cancels Flash Flood Warning for Bell Harla...</td>\n      <td>1</td>\n      <td>jkl cancels flash flood warning bell harlan kn...</td>\n      <td>85</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1792</th>\n      <td>2571</td>\n      <td>crash</td>\n      <td>Melbourne, Australia</td>\n      <td>#INCIDENT\\nCrash in Pascoe Vale South outbound...</td>\n      <td>1</td>\n      <td>#incident crash pascoe vale south outbound tul...</td>\n      <td>136</td>\n      <td>21</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.1</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5741</th>\n      <td>8195</td>\n      <td>riot</td>\n      <td>Belgrade</td>\n      <td>To All The Meat-Loving Feminists Of The World ...</td>\n      <td>0</td>\n      <td>meatloving feminists world riot grill arrived ...</td>\n      <td>135</td>\n      <td>21</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>10</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Test set Feature engineering\n- now apply the same to the test set","metadata":{}},{"cell_type":"code","source":"test_df['text_length'] = test_df['text'].apply(len)\ntest_df['word_count'] = test_df['text'].apply(lambda x: len(x.split()))\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('#')]))\ntest_df['mention_count'] = test_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('@')]))\ntest_df['has_url'] = test_df['text'].apply(lambda x: 1 if re.search(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", x) else 0)\ntest_df['sentiment_score'] = test_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ntest_df['noun_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'NOUN' or token.pos_ == 'PROPN']))\ntest_df['verb_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'VERB']))\ntest_df['adverb_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADV']))\ntest_df['adjective_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADJ']))\ntest_df['profanity_count'] = test_df['text'].apply(lambda x: len([w for w in x if w in profanity.CENSOR_WORDSET]))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:15:43.493317Z","iopub.execute_input":"2024-07-29T20:15:43.494012Z","iopub.status.idle":"2024-07-29T20:19:23.998548Z","shell.execute_reply.started":"2024-07-29T20:15:43.493981Z","shell.execute_reply":"2024-07-29T20:19:23.997708Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:19:23.999842Z","iopub.execute_input":"2024-07-29T20:19:24.000616Z","iopub.status.idle":"2024-07-29T20:19:24.016835Z","shell.execute_reply.started":"2024-07-29T20:19:24.000582Z","shell.execute_reply":"2024-07-29T20:19:24.015828Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         id              keyword           location  \\\n3160  10491             wildfire           Colorado   \n1945   6562               injury                NaN   \n532    1742  buildings%20burning                NaN   \n1382   4558     emergency%20plan  Vancouver, Canada   \n1512   5031           eyewitness                NaN   \n\n                                                   text  \\\n3160  11:57am Wildfire by The Mynabirds from Lovers ...   \n1945  nflweek1picks: Michael Floyd's hand injury sho...   \n532     kou is like [CASH REGISTER] [BUILDINGS BURNING]   \n1382  Calgary takes another beating from summer stor...   \n1512  How 'Little Boy' Affected the People In Hirosh...   \n\n                                             clean_text  text_length  \\\n3160              1157am wildfire mynabirds lovers know           50   \n1945  nflweek1picks michael floyds hand injury shoul...          121   \n532            kou like cash register buildings burning           47   \n1382  calgary takes another beating summer storms ci...          141   \n1512  little boy affected people hiroshima eyewitnes...          102   \n\n      word_count  hashtag_count  mention_count  has_url  sentiment_score  \\\n3160           8              0              0        0         0.000000   \n1945          16              0              0        0         0.000000   \n532            7              0              0        0         0.000000   \n1382          19              0              0        1         0.000000   \n1512          13              0              0        1        -0.234375   \n\n      noun_count  verb_count  adverb_count  adjective_count  profanity_count  \n3160           4           1             0                0                0  \n1945           9           4             0                0                0  \n532            2           2             0                0                0  \n1382          11           3             0                0                0  \n1512           6           1             0                1                0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>has_url</th>\n      <th>sentiment_score</th>\n      <th>noun_count</th>\n      <th>verb_count</th>\n      <th>adverb_count</th>\n      <th>adjective_count</th>\n      <th>profanity_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3160</th>\n      <td>10491</td>\n      <td>wildfire</td>\n      <td>Colorado</td>\n      <td>11:57am Wildfire by The Mynabirds from Lovers ...</td>\n      <td>1157am wildfire mynabirds lovers know</td>\n      <td>50</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1945</th>\n      <td>6562</td>\n      <td>injury</td>\n      <td>NaN</td>\n      <td>nflweek1picks: Michael Floyd's hand injury sho...</td>\n      <td>nflweek1picks michael floyds hand injury shoul...</td>\n      <td>121</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>9</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>532</th>\n      <td>1742</td>\n      <td>buildings%20burning</td>\n      <td>NaN</td>\n      <td>kou is like [CASH REGISTER] [BUILDINGS BURNING]</td>\n      <td>kou like cash register buildings burning</td>\n      <td>47</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1382</th>\n      <td>4558</td>\n      <td>emergency%20plan</td>\n      <td>Vancouver, Canada</td>\n      <td>Calgary takes another beating from summer stor...</td>\n      <td>calgary takes another beating summer storms ci...</td>\n      <td>141</td>\n      <td>19</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1512</th>\n      <td>5031</td>\n      <td>eyewitness</td>\n      <td>NaN</td>\n      <td>How 'Little Boy' Affected the People In Hirosh...</td>\n      <td>little boy affected people hiroshima eyewitnes...</td>\n      <td>102</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.234375</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## TF-IDF Vectorization\n- Convert the cleaned text data into numerical features using TF-IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize TF-IDF Vectorizer\ntfidf = TfidfVectorizer(max_features=10000)\n\n# Fit and transform the training data\nX_train_tfidf = tfidf.fit_transform(train_df['clean_text'])\n\n# Transform the test data\nX_test_tfidf = tfidf.transform(test_df['clean_text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:19:24.018337Z","iopub.execute_input":"2024-07-29T20:19:24.018717Z","iopub.status.idle":"2024-07-29T20:19:24.231043Z","shell.execute_reply.started":"2024-07-29T20:19:24.018684Z","shell.execute_reply":"2024-07-29T20:19:24.230381Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## BERT Embeddings\n- Generate BERT embeddings for the text data","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer\nimport torch\n\n#load tokenizer and BERT model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n#tokenize and encode the text\n# Tokenize and encode the text\ndef get_bert_embeddings(text_list):\n   inputs = tokenizer(text_list, return_tensors='pt', padding=True, truncation=True, max_length=512)\n   with torch.no_grad():\n      outputs = bert_model(**inputs)\n   return outputs.last_hidden_state[:, 0, :].numpy()\n\n# Get BERT embeddings for train and test data\nX_train_bert = get_bert_embeddings(train_df['clean_text'].tolist())\nX_test_bert = get_bert_embeddings(test_df['clean_text'].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:19:24.232051Z","iopub.execute_input":"2024-07-29T20:19:24.232349Z","iopub.status.idle":"2024-07-29T20:30:27.651663Z","shell.execute_reply.started":"2024-07-29T20:19:24.232313Z","shell.execute_reply":"2024-07-29T20:30:27.650544Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d73dc2b5a04261aa4096185f8d4651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0c9739e5304a6f92861ef708413d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6bc9e540d4b41e28d4ccf339f956422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3877531b57174dfeb55f58fd8fb9d2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11300446f2d24144a25c6134cda5c738"}},"metadata":{}}]},{"cell_type":"code","source":"X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_bert, \n                              train_df[['text_length', 'word_count', 'hashtag_count', 'mention_count', 'has_url', \n                                       'sentiment_score', 'noun_count', 'verb_count', 'adverb_count', 'adjective_count', 'profanity_count']].values))\n\nX_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_bert, \n                              test_df[['text_length', 'word_count', 'hashtag_count', 'mention_count', 'has_url', \n                                       'sentiment_score', 'noun_count', 'verb_count', 'adverb_count', 'adjective_count', 'profanity_count']].values))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:30:27.653063Z","iopub.execute_input":"2024-07-29T20:30:27.653663Z","iopub.status.idle":"2024-07-29T20:30:28.480622Z","shell.execute_reply.started":"2024-07-29T20:30:27.653635Z","shell.execute_reply":"2024-07-29T20:30:28.479817Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection\n- here we test a variety of models and choose a few to fine tune based on classification report ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom sklearn.svm import SVC\n\n# Split and scale the data\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, train_df['target'], test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train_split_scaled = scaler.fit_transform(X_train_split)\nX_val_split_scaled = scaler.transform(X_val_split)\n\n# Define the SVM model\nmodel = SVC(kernel='rbf')\n\n\nmodel.fit(X_train_split_scaled, y_train_split)\ny_pred = model.predict(X_val_split_scaled)\n\nprint(f\"Model: SVM\")\nprint(classification_report(y_val_split, y_pred))\nprint('-' * 60)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:30:28.481803Z","iopub.execute_input":"2024-07-29T20:30:28.482100Z","iopub.status.idle":"2024-07-29T20:37:13.208147Z","shell.execute_reply.started":"2024-07-29T20:30:28.482075Z","shell.execute_reply":"2024-07-29T20:37:13.207066Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: SVM\n              precision    recall  f1-score   support\n\n           0       0.78      0.93      0.85       874\n           1       0.87      0.64      0.74       649\n\n    accuracy                           0.81      1523\n   macro avg       0.82      0.78      0.79      1523\nweighted avg       0.82      0.81      0.80      1523\n\n------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"hyperparameter_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n\ngrid_search = GridSearchCV(model, hyperparameter_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train_split_scaled, y_train_split)\nprint(\"Best Parameters for SVM:\", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T20:37:13.209559Z","iopub.execute_input":"2024-07-29T20:37:13.210188Z","iopub.status.idle":"2024-07-29T22:39:15.383284Z","shell.execute_reply.started":"2024-07-29T20:37:13.210151Z","shell.execute_reply":"2024-07-29T22:39:15.382029Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Best Parameters for SVM: {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Re-evaluate with best Hyperparameters","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = SVC(C=1, gamma='auto', kernel='rbf')\n\nmodel.fit(X_train_split_scaled, y_train_split)\ny_pred = model.predict(X_val_split_scaled)\n\nprint(f\"Model: SVM\")\nprint(classification_report(y_val_split, y_pred))\nprint('-' * 60)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T22:39:15.384948Z","iopub.execute_input":"2024-07-29T22:39:15.385277Z","iopub.status.idle":"2024-07-29T22:45:40.593644Z","shell.execute_reply.started":"2024-07-29T22:39:15.385247Z","shell.execute_reply":"2024-07-29T22:45:40.592276Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: SVM\n              precision    recall  f1-score   support\n\n           0       0.78      0.92      0.85       874\n           1       0.86      0.65      0.74       649\n\n    accuracy                           0.81      1523\n   macro avg       0.82      0.79      0.79      1523\nweighted avg       0.81      0.81      0.80      1523\n\n------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Final Submission","metadata":{}},{"cell_type":"code","source":"X_train = X_train_combined\nX_test = X_test_combined\ny_train = train_df['target']\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel = SVC(C=1, gamma='auto', kernel='rbf')\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'id' : test_df['id'],\n    'target' : y_pred\n})\n\nsubmission.to_csv('svm_submission.csv', index=False)\n\nprint('Submission created successfully')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T22:46:55.202178Z","iopub.execute_input":"2024-07-29T22:46:55.202597Z","iopub.status.idle":"2024-07-29T22:59:28.801575Z","shell.execute_reply.started":"2024-07-29T22:46:55.202567Z","shell.execute_reply":"2024-07-29T22:59:28.800313Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Submission created successfully\n","output_type":"stream"}]}]}