{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facciola NLP Disaster Tweet Model\n",
    "\n",
    "- In this competition we are building an NLP model to predict whether a Tweet is about a real disaster or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DATA_DIR = os.path.join(os.getcwd(),'data') if os.environ['COMPUTERNAME'] == 'NFACCIOL-MOBL' else \"/kaggle/input/nlp-getting-started/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "- examine the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "None\n",
      "\n",
      "Test set info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set info\")\n",
    "print(train_df.info())\n",
    "print()\n",
    "print(\"Test set info\")\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- here we clean the text data by removing unneccssary characters, handling missing values, and normalizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nfacciol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive #wildfires evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \n",
       "0       1      deeds reason #earthquake may allah forgive us  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  residents asked shelter place notified officer...  \n",
       "3       1  13000 people receive #wildfires evacuation ord...  \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.set_proxy('http://proxy-dmz.intel.com:911/')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "\n",
    "def clean_text(text):\n",
    "   #remove URLS\n",
    "   text = re.sub(r'http\\S+', '', text)\n",
    "   #remove HTML tags\n",
    "   text = re.sub(r'<.*?>', '', text)\n",
    "   # Remove non-alphanumeric characters except hashtags and mentions\n",
    "   text = re.sub(r'[^a-zA-Z0-9\\s#@]', '', text)\n",
    "   # Convert to lowercase\n",
    "   text = text.lower()\n",
    "   # Remove stopwords\n",
    "   text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "   return text\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- **text length**: Calculate the length of each tweet. This can help capture information about tweet complexity or verbosity.\n",
    "- **word count**: Count the number of words in each tweet, which may provide insight into tweet structure.\n",
    "- **hashtag count**: Count the number of hashtags in each tweet, as this can be indicative of topic relevance or trending discussions.\n",
    "- **mention count**: Count the number of user mentions, which can indicate the tweet's engagement level.\n",
    "- **hasUrl**: Create a binary feature indicating whether the tweet contains a URL.\n",
    "- **sentiment score**: Use a pre-trained sentiment analyzer to get a sentiment score for each tweet.\n",
    "- **pos tags**: Count the occurrence of different parts of speech in each tweet.\n",
    "- **profanity count**: Count the number of profane words in each tweet using a predefined list of profane words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>3168</td>\n",
       "      <td>deluge</td>\n",
       "      <td>617-BTOWN-BEATDOWN</td>\n",
       "      <td>Photo: forrestmankins: Colorado camping. http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>photo forrestmankins colorado camping</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1083</td>\n",
       "      <td>blew%20up</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>My Instagram just blew up apparently I was fea...</td>\n",
       "      <td>0</td>\n",
       "      <td>instagram blew apparently featured jazz tonigh...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>9402</td>\n",
       "      <td>survivors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear @POTUS In the name of humanityI apologize...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear @potus name humanityi apologized #hiroshi...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>2680</td>\n",
       "      <td>crush</td>\n",
       "      <td>Washington, DC NATIVE</td>\n",
       "      <td>#MrRobinson is giving me #TheSteveHarveyShow v...</td>\n",
       "      <td>0</td>\n",
       "      <td>#mrrobinson giving #thesteveharveyshow vibe mu...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2908</td>\n",
       "      <td>danger</td>\n",
       "      <td>ayr</td>\n",
       "      <td>Danger of union bears http://t.co/lhdcpNZx6A</td>\n",
       "      <td>0</td>\n",
       "      <td>danger union bears</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword               location  \\\n",
       "2212  3168     deluge     617-BTOWN-BEATDOWN   \n",
       "751   1083  blew%20up                Indiana   \n",
       "6569  9402  survivors                    NaN   \n",
       "1865  2680      crush  Washington, DC NATIVE   \n",
       "2025  2908     danger                    ayr   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2212  Photo: forrestmankins: Colorado camping. http:...       0   \n",
       "751   My Instagram just blew up apparently I was fea...       0   \n",
       "6569  Dear @POTUS In the name of humanityI apologize...       1   \n",
       "1865  #MrRobinson is giving me #TheSteveHarveyShow v...       0   \n",
       "2025       Danger of union bears http://t.co/lhdcpNZx6A       0   \n",
       "\n",
       "                                             clean_text  text_length  \n",
       "2212              photo forrestmankins colorado camping           63  \n",
       "751   instagram blew apparently featured jazz tonigh...           99  \n",
       "6569  dear @potus name humanityi apologized #hiroshi...          136  \n",
       "1865  #mrrobinson giving #thesteveharveyshow vibe mu...          137  \n",
       "2025                                 danger union bears           44  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>7607</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'll be at SFA very soon....#Pandemonium http:...</td>\n",
       "      <td>1</td>\n",
       "      <td>ill sfa soon#pandemonium</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>4043</td>\n",
       "      <td>disaster</td>\n",
       "      <td>USA</td>\n",
       "      <td>DISASTER AVERTED: Police kill gunman with Û÷h...</td>\n",
       "      <td>0</td>\n",
       "      <td>disaster averted police kill gunman hoax devic...</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>2702</td>\n",
       "      <td>crush</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kenny holland crush da vida</td>\n",
       "      <td>0</td>\n",
       "      <td>kenny holland crush da vida</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>121</td>\n",
       "      <td>accident</td>\n",
       "      <td>South Bloomfield, OH</td>\n",
       "      <td>Accident in #Ashville on US 23 SB before SR 75...</td>\n",
       "      <td>1</td>\n",
       "      <td>accident #ashville us 23 sb sr 752 #traffic</td>\n",
       "      <td>79</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>7698</td>\n",
       "      <td>panicking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Dirk_NoMissSki yea but if someone faints why ...</td>\n",
       "      <td>1</td>\n",
       "      <td>@dirknomissski yea someone faints panicking th...</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword              location  \\\n",
       "5328  7607  pandemonium                   NaN   \n",
       "2811  4043     disaster                  USA    \n",
       "1881  2702        crush                   NaN   \n",
       "84     121     accident  South Bloomfield, OH   \n",
       "5396  7698    panicking                   NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "5328  I'll be at SFA very soon....#Pandemonium http:...       1   \n",
       "2811  DISASTER AVERTED: Police kill gunman with Û÷h...       0   \n",
       "1881                        kenny holland crush da vida       0   \n",
       "84    Accident in #Ashville on US 23 SB before SR 75...       1   \n",
       "5396  @Dirk_NoMissSki yea but if someone faints why ...       1   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "5328                           ill sfa soon#pandemonium           63   \n",
       "2811  disaster averted police kill gunman hoax devic...           93   \n",
       "1881                        kenny holland crush da vida           27   \n",
       "84          accident #ashville us 23 sb sr 752 #traffic           79   \n",
       "5396  @dirknomissski yea someone faints panicking th...           88   \n",
       "\n",
       "      word_count  \n",
       "5328           7  \n",
       "2811          10  \n",
       "1881           5  \n",
       "84            12  \n",
       "5396          14  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>3058</td>\n",
       "      <td>deaths</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This why BSF Jawans died Fidayeen has AKs and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>bsf jawans died fidayeen aks bloody #insas ins...</td>\n",
       "      <td>139</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>2643</td>\n",
       "      <td>crashed</td>\n",
       "      <td>Kingswinford</td>\n",
       "      <td>I just nearly crashed my car typing 'Paul Rudd...</td>\n",
       "      <td>0</td>\n",
       "      <td>nearly crashed car typing paul rudd attacked f...</td>\n",
       "      <td>95</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>7001</td>\n",
       "      <td>mayhem</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>I guess ill never be able to go to mayhem...</td>\n",
       "      <td>0</td>\n",
       "      <td>guess ill never able go mayhem</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>6695</td>\n",
       "      <td>landslide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Landslide kills three near Venice after heavyå...</td>\n",
       "      <td>1</td>\n",
       "      <td>landslide kills three near venice heavyrain</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>6527</td>\n",
       "      <td>injuries</td>\n",
       "      <td>Georgia, U.S.A.</td>\n",
       "      <td>@msnbc What a fucking idiot. He had a gun &amp;amp...</td>\n",
       "      <td>1</td>\n",
       "      <td>@msnbc fucking idiot gun amp hatchet yet still...</td>\n",
       "      <td>127</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword         location  \\\n",
       "2130  3058     deaths              NaN   \n",
       "1838  2643    crashed     Kingswinford   \n",
       "4916  7001     mayhem      Orlando, FL   \n",
       "4709  6695  landslide              NaN   \n",
       "4589  6527   injuries  Georgia, U.S.A.   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2130  This why BSF Jawans died Fidayeen has AKs and ...       1   \n",
       "1838  I just nearly crashed my car typing 'Paul Rudd...       0   \n",
       "4916       I guess ill never be able to go to mayhem...       0   \n",
       "4709  Landslide kills three near Venice after heavyå...       1   \n",
       "4589  @msnbc What a fucking idiot. He had a gun &amp...       1   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "2130  bsf jawans died fidayeen aks bloody #insas ins...          139   \n",
       "1838  nearly crashed car typing paul rudd attacked f...           95   \n",
       "4916                     guess ill never able go mayhem           44   \n",
       "4709        landslide kills three near venice heavyrain           74   \n",
       "4589  @msnbc fucking idiot gun amp hatchet yet still...          127   \n",
       "\n",
       "      word_count  hashtag_count  \n",
       "2130          22              1  \n",
       "1838          18              0  \n",
       "4916          10              0  \n",
       "4709           8              0  \n",
       "4589          23              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['hashtag_count'] = train_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('#')]))\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4332</th>\n",
       "      <td>6152</td>\n",
       "      <td>hijack</td>\n",
       "      <td>Port Harcourt, Nigeria</td>\n",
       "      <td>Plans by former First Lady and wife of ex-Pres...</td>\n",
       "      <td>1</td>\n",
       "      <td>plans former first lady wife expresident goodl...</td>\n",
       "      <td>136</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>194</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>304</td>\n",
       "      <td>'The first man gets the oyster the second man ...</td>\n",
       "      <td>0</td>\n",
       "      <td>first man gets oyster second man gets shell an...</td>\n",
       "      <td>78</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>7081</td>\n",
       "      <td>meltdown</td>\n",
       "      <td>Leeds, England</td>\n",
       "      <td>Pam's Barry Island wedding meltdown ??????????</td>\n",
       "      <td>0</td>\n",
       "      <td>pams barry island wedding meltdown</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>8447</td>\n",
       "      <td>screamed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I JUST SCREAMED SIDJSJDJEKDJSKDJD . I CANT STA...</td>\n",
       "      <td>0</td>\n",
       "      <td>screamed sidjsjdjekdjskdjd cant stand</td>\n",
       "      <td>79</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>1604</td>\n",
       "      <td>bombed</td>\n",
       "      <td>Atlanta Georgia</td>\n",
       "      <td>@WhiteHouse @POTUS Just cos Germany invaded Po...</td>\n",
       "      <td>1</td>\n",
       "      <td>@whitehouse @potus cos germany invaded poland ...</td>\n",
       "      <td>119</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword                location  \\\n",
       "4332  6152      hijack  Port Harcourt, Nigeria   \n",
       "134    194  aftershock                     304   \n",
       "4969  7081    meltdown          Leeds, England   \n",
       "5916  8447    screamed                     NaN   \n",
       "1111  1604      bombed         Atlanta Georgia   \n",
       "\n",
       "                                                   text  target  \\\n",
       "4332  Plans by former First Lady and wife of ex-Pres...       1   \n",
       "134   'The first man gets the oyster the second man ...       0   \n",
       "4969     Pam's Barry Island wedding meltdown ??????????       0   \n",
       "5916  I JUST SCREAMED SIDJSJDJEKDJSKDJD . I CANT STA...       0   \n",
       "1111  @WhiteHouse @POTUS Just cos Germany invaded Po...       1   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "4332  plans former first lady wife expresident goodl...          136   \n",
       "134   first man gets oyster second man gets shell an...           78   \n",
       "4969                 pams barry island wedding meltdown           46   \n",
       "5916              screamed sidjsjdjekdjskdjd cant stand           79   \n",
       "1111  @whitehouse @potus cos germany invaded poland ...          119   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  \n",
       "4332          19              0              0  \n",
       "134           14              0              0  \n",
       "4969           6              0              0  \n",
       "5916          11              0              0  \n",
       "1111          14              0              2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['mention_count'] = train_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('@')]))\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7443</th>\n",
       "      <td>10651</td>\n",
       "      <td>wounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white ppl bruise easily.. where ur bullet woun...</td>\n",
       "      <td>0</td>\n",
       "      <td>white ppl bruise easily ur bullet wounds</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>4456</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>New York</td>\n",
       "      <td>Woman electrocuted #Red #Redblood #videoclip h...</td>\n",
       "      <td>0</td>\n",
       "      <td>woman electrocuted #red #redblood #videoclip #</td>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their...</td>\n",
       "      <td>1</td>\n",
       "      <td>build army 100 dogs leader lion dogs fight lik...</td>\n",
       "      <td>96</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1650</td>\n",
       "      <td>bombing</td>\n",
       "      <td>SWMO</td>\n",
       "      <td>Japan Marks 70th Anniversary of Hiroshima Atom...</td>\n",
       "      <td>1</td>\n",
       "      <td>japan marks 70th anniversary hiroshima atomic ...</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>3283</td>\n",
       "      <td>demolish</td>\n",
       "      <td>us-east-1a</td>\n",
       "      <td>Read this already in '14 but it was and remain...</td>\n",
       "      <td>0</td>\n",
       "      <td>read already 14 remains one favorite articles ...</td>\n",
       "      <td>138</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       keyword    location  \\\n",
       "7443  10651        wounds         NaN   \n",
       "3105   4456  electrocuted    New York   \n",
       "356     513          army      Studio   \n",
       "1146   1650       bombing        SWMO   \n",
       "2288   3283      demolish  us-east-1a   \n",
       "\n",
       "                                                   text  target  \\\n",
       "7443  white ppl bruise easily.. where ur bullet woun...       0   \n",
       "3105  Woman electrocuted #Red #Redblood #videoclip h...       0   \n",
       "356   But if you build an army of 100 dogs and their...       1   \n",
       "1146  Japan Marks 70th Anniversary of Hiroshima Atom...       1   \n",
       "2288  Read this already in '14 but it was and remain...       0   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "7443           white ppl bruise easily ur bullet wounds           80   \n",
       "3105     woman electrocuted #red #redblood #videoclip #           69   \n",
       "356   build army 100 dogs leader lion dogs fight lik...           96   \n",
       "1146  japan marks 70th anniversary hiroshima atomic ...           79   \n",
       "2288  read already 14 remains one favorite articles ...          138   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  has_url  \n",
       "7443          11              0              0        1  \n",
       "3105           7              4              0        1  \n",
       "356           22              0              0        0  \n",
       "1146           9              0              0        1  \n",
       "2288          21              0              0        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['has_url'] = train_df['text'].apply(lambda x: 1 if re.search(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", x) else 0)\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The end!</td>\n",
       "      <td>0</td>\n",
       "      <td>end</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>9571</td>\n",
       "      <td>thunder</td>\n",
       "      <td>Gander NF</td>\n",
       "      <td>Random wind gust just came through #Gander.  P...</td>\n",
       "      <td>1</td>\n",
       "      <td>random wind gust came #gander probably convect...</td>\n",
       "      <td>139</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>9838</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Esteemed journalist recalls tragic effects of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>esteemed journalist recalls tragic effects una...</td>\n",
       "      <td>140</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>4355</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>oklahoma</td>\n",
       "      <td>Posted a new song: 'Earthquake' http://t.co/Rf...</td>\n",
       "      <td>0</td>\n",
       "      <td>posted new song earthquake</td>\n",
       "      <td>77</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>2198</td>\n",
       "      <td>catastrophic</td>\n",
       "      <td>Planet Earth</td>\n",
       "      <td>Learning from the Legacy of a Catastrophic Eru...</td>\n",
       "      <td>1</td>\n",
       "      <td>learning legacy catastrophic eruption new yorker</td>\n",
       "      <td>91</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword       location  \\\n",
       "30      44           NaN            NaN   \n",
       "6679  9571       thunder      Gander NF   \n",
       "6866  9838        trauma  Nashville, TN   \n",
       "3034  4355    earthquake       oklahoma   \n",
       "1522  2198  catastrophic   Planet Earth   \n",
       "\n",
       "                                                   text  target  \\\n",
       "30                                             The end!       0   \n",
       "6679  Random wind gust just came through #Gander.  P...       1   \n",
       "6866  Esteemed journalist recalls tragic effects of ...       1   \n",
       "3034  Posted a new song: 'Earthquake' http://t.co/Rf...       0   \n",
       "1522  Learning from the Legacy of a Catastrophic Eru...       1   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "30                                                  end            8   \n",
       "6679  random wind gust came #gander probably convect...          139   \n",
       "6866  esteemed journalist recalls tragic effects una...          140   \n",
       "3034                         posted new song earthquake           77   \n",
       "1522   learning legacy catastrophic eruption new yorker           91   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  has_url  sentiment_score  \n",
       "30             2              0              0        0         0.000000  \n",
       "6679          23              2              0        0        -0.350000  \n",
       "6866          13              2              3        1        -0.750000  \n",
       "3034           7              0              0        1         0.136364  \n",
       "1522          13              0              0        1         0.136364  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "train_df['sentiment_score'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>3131</td>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aircraft debris found on island is from MH370 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>aircraft debris found island mh370 malaysia co...</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>1153</td>\n",
       "      <td>blight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://t.co/ETkd58Un8n - Cleveland Heights Sha...</td>\n",
       "      <td>0</td>\n",
       "      <td>cleveland heights shaker heights fight blight ...</td>\n",
       "      <td>114</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>212</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>#KCA #VoteJKT48ID mbataweel: #RIP #BINLADEN Fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>#kca #votejkt48id mbataweel #rip #binladen fam...</td>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>9030</td>\n",
       "      <td>stretcher</td>\n",
       "      <td>??</td>\n",
       "      <td>Stretcher in 5 min // Speaker Deck http://t.co...</td>\n",
       "      <td>0</td>\n",
       "      <td>stretcher 5 min speaker deck</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>10101</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>The Peach State</td>\n",
       "      <td>I think a Typhoon just passed through here lol</td>\n",
       "      <td>1</td>\n",
       "      <td>think typhoon passed lol</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword         location  \\\n",
       "2184   3131               debris              NaN   \n",
       "795    1153               blight              NaN   \n",
       "148     212  airplane%20accident        Indonesia   \n",
       "6318   9030            stretcher               ??   \n",
       "7050  10101              typhoon  The Peach State   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2184  Aircraft debris found on island is from MH370 ...       1   \n",
       "795   http://t.co/ETkd58Un8n - Cleveland Heights Sha...       0   \n",
       "148   #KCA #VoteJKT48ID mbataweel: #RIP #BINLADEN Fa...       1   \n",
       "6318  Stretcher in 5 min // Speaker Deck http://t.co...       0   \n",
       "7050     I think a Typhoon just passed through here lol       1   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "2184  aircraft debris found island mh370 malaysia co...           86   \n",
       "795   cleveland heights shaker heights fight blight ...          114   \n",
       "148   #kca #votejkt48id mbataweel #rip #binladen fam...           95   \n",
       "6318                       stretcher 5 min speaker deck           57   \n",
       "7050                           think typhoon passed lol           46   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  has_url  sentiment_score  \\\n",
       "2184          11              0              0        1              0.0   \n",
       "795           13              0              0        1              0.0   \n",
       "148           13              4              0        0             -0.2   \n",
       "6318           8              0              0        1              0.0   \n",
       "7050           9              0              0        0              0.8   \n",
       "\n",
       "      noun_count  verb_count  adverb_count  adjective_count  \n",
       "2184           6           2             0                0  \n",
       "795           10           1             0                0  \n",
       "148            8           2             0                0  \n",
       "6318           4           0             0                1  \n",
       "7050           2           2             2                0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "train_df['noun_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'NOUN' or token.pos_ == 'PROPN']))\n",
    "train_df['verb_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'VERB']))\n",
    "train_df['adverb_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADV']))\n",
    "train_df['adjective_count'] = train_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADJ']))\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>profanity_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>3831</td>\n",
       "      <td>detonate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@WoundedPigeon http://t.co/s9soAeVcVo Detonate...</td>\n",
       "      <td>0</td>\n",
       "      <td>@woundedpigeon detonate @apollobrown ft mop</td>\n",
       "      <td>73</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>7355</td>\n",
       "      <td>obliterate</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>@klavierstuk doesn't so LVG is forced into the...</td>\n",
       "      <td>0</td>\n",
       "      <td>@klavierstuk doesnt lvg forced market may beat...</td>\n",
       "      <td>139</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>363</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>United States</td>\n",
       "      <td>Are souls punished withåÊannihilation? http://...</td>\n",
       "      <td>0</td>\n",
       "      <td>souls punished withannihilation</td>\n",
       "      <td>84</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>3263</td>\n",
       "      <td>demolish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@kirkmin after listening to you demolish @Bart...</td>\n",
       "      <td>0</td>\n",
       "      <td>@kirkmin listening demolish @barthubbuch @weei...</td>\n",
       "      <td>135</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>three days work theyve pretty much wrecked hah...</td>\n",
       "      <td>107</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       keyword           location  \\\n",
       "2669   3831      detonate                NaN   \n",
       "5156   7355    obliterate     United Kingdom   \n",
       "255     363  annihilation      United States   \n",
       "2275   3263      demolish                NaN   \n",
       "7579  10831       wrecked  Vancouver, Canada   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2669  @WoundedPigeon http://t.co/s9soAeVcVo Detonate...       0   \n",
       "5156  @klavierstuk doesn't so LVG is forced into the...       0   \n",
       "255   Are souls punished withåÊannihilation? http://...       0   \n",
       "2275  @kirkmin after listening to you demolish @Bart...       0   \n",
       "7579  Three days off from work and they've pretty mu...       0   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "2669        @woundedpigeon detonate @apollobrown ft mop           73   \n",
       "5156  @klavierstuk doesnt lvg forced market may beat...          139   \n",
       "255                     souls punished withannihilation           84   \n",
       "2275  @kirkmin listening demolish @barthubbuch @weei...          135   \n",
       "7579  three days work theyve pretty much wrecked hah...          107   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  has_url  sentiment_score  \\\n",
       "2669           7              0              2        1         0.000000   \n",
       "5156          25              0              1        0        -0.075000   \n",
       "255            6              0              0        1         0.000000   \n",
       "2275          22              0              3        0         0.000000   \n",
       "7579          20              0              0        0         0.216667   \n",
       "\n",
       "      noun_count  verb_count  adverb_count  adjective_count  profanity_count  \n",
       "2669           5           0             0                0                0  \n",
       "5156           9           3             1                2                0  \n",
       "255            2           1             0                0                0  \n",
       "2275           9           3             0                1                0  \n",
       "7579           5           1             3                0                0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from better_profanity import profanity\n",
    "\n",
    "train_df['profanity_count'] = train_df['text'].apply(lambda x: len([w for w in x if w in profanity.CENSOR_WORDSET]))\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set Feature engineering\n",
    "- now apply the same to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text_length'] = test_df['text'].apply(len)\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(x.split()))\n",
    "test_df['hashtag_count'] = test_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('#')]))\n",
    "test_df['mention_count'] = test_df['text'].apply(lambda x: len([w for w in x.split() if w.startswith('@')]))\n",
    "test_df['has_url'] = test_df['text'].apply(lambda x: 1 if re.search(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", x) else 0)\n",
    "test_df['sentiment_score'] = test_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "test_df['noun_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'NOUN' or token.pos_ == 'PROPN']))\n",
    "test_df['verb_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'VERB']))\n",
    "test_df['adverb_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADV']))\n",
    "test_df['adjective_count'] = test_df['text'].apply(lambda x: len([token.pos_ for token in nlp(x) if token.pos_ == 'ADJ']))\n",
    "test_df['profanity_count'] = test_df['text'].apply(lambda x: len([w for w in x if w in profanity.CENSOR_WORDSET]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>profanity_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2716</td>\n",
       "      <td>crushed</td>\n",
       "      <td>Motown, WV</td>\n",
       "      <td>So in one episode they undo season 1. Kai join...</td>\n",
       "      <td>one episode undo season 1 kai joins ff ren bea...</td>\n",
       "      <td>117</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1127</td>\n",
       "      <td>blew%20up</td>\n",
       "      <td>twitch.tv/dgn_esports</td>\n",
       "      <td>The only reason why player's now have an ego i...</td>\n",
       "      <td>reason players ego cause mw3 cod champs thats ...</td>\n",
       "      <td>140</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1056</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>Louisville, KY</td>\n",
       "      <td>Looks like Reynolds and Montano coming in. Nee...</td>\n",
       "      <td>looks like reynolds montano coming need stop b...</td>\n",
       "      <td>102</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>5774</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q: Why do ducks have big flat feet? A: To stam...</td>\n",
       "      <td>q ducks big flat feet stamp forest fires q ele...</td>\n",
       "      <td>136</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>7575</td>\n",
       "      <td>outbreak</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
       "      <td>families sue legionnaires 40 families affected...</td>\n",
       "      <td>136</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         keyword               location  \\\n",
       "826   2716         crushed             Motown, WV   \n",
       "348   1127       blew%20up  twitch.tv/dgn_esports   \n",
       "325   1056        bleeding         Louisville, KY   \n",
       "1711  5774  forest%20fires                    NaN   \n",
       "2272  7575        outbreak                  Dubai   \n",
       "\n",
       "                                                   text  \\\n",
       "826   So in one episode they undo season 1. Kai join...   \n",
       "348   The only reason why player's now have an ego i...   \n",
       "325   Looks like Reynolds and Montano coming in. Nee...   \n",
       "1711  Q: Why do ducks have big flat feet? A: To stam...   \n",
       "2272  Families to sue over Legionnaires: More than 4...   \n",
       "\n",
       "                                             clean_text  text_length  \\\n",
       "826   one episode undo season 1 kai joins ff ren bea...          117   \n",
       "348   reason players ego cause mw3 cod champs thats ...          140   \n",
       "325   looks like reynolds montano coming need stop b...          102   \n",
       "1711  q ducks big flat feet stamp forest fires q ele...          136   \n",
       "2272  families sue legionnaires 40 families affected...          136   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  has_url  sentiment_score  \\\n",
       "826           23              0              0        0          -0.2500   \n",
       "348           27              0              0        0           0.0000   \n",
       "325           18              0              0        0           0.5500   \n",
       "1711          28              0              0        0          -0.0125   \n",
       "2272          18              0              0        1           0.5000   \n",
       "\n",
       "      noun_count  verb_count  adverb_count  adjective_count  profanity_count  \n",
       "826            9           4             1                0                0  \n",
       "348           10           3             3                1                0  \n",
       "325            4           4             2                1                0  \n",
       "1711          11           5             0                4                0  \n",
       "2272           7           2             0                2                0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "- Convert the cleaned text data into numerical features using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['clean_text'])\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf.transform(test_df['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings\n",
    "- Generate BERT embeddings for the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "bert_path = os.path.join(os.getcwd(), 'bert-base-uncased')\n",
    "\n",
    "#load tokenizer and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path, local_files_only=True)\n",
    "bert_model = BertModel.from_pretrained(bert_path, local_files_only=True)\n",
    "\n",
    "#tokenize and encode the text\n",
    "# Tokenize and encode the text\n",
    "def get_bert_embeddings(text_list):\n",
    "   inputs = tokenizer(text_list, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "   with torch.no_grad():\n",
    "      outputs = bert_model(**inputs)\n",
    "   return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Get BERT embeddings for train and test data\n",
    "X_train_bert = get_bert_embeddings(train_df['clean_text'].tolist())\n",
    "X_test_bert = get_bert_embeddings(test_df['clean_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_bert, \n",
    "                              train_df[['text_length', 'word_count', 'hashtag_count', 'mention_count', 'has_url', \n",
    "                                       'sentiment_score', 'noun_count', 'verb_count', 'adverb_count', 'adjective_count', 'profanity_count']].values))\n",
    "\n",
    "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_bert, \n",
    "                              test_df[['text_length', 'word_count', 'hashtag_count', 'mention_count', 'has_url', \n",
    "                                       'sentiment_score', 'noun_count', 'verb_count', 'adverb_count', 'adjective_count', 'profanity_count']].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "- here we test a variety of models and choose a few to fine tune based on classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78       874\n",
      "           1       0.71      0.71      0.71       649\n",
      "\n",
      "    accuracy                           0.75      1523\n",
      "   macro avg       0.75      0.75      0.75      1523\n",
      "weighted avg       0.75      0.75      0.75      1523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85       874\n",
      "           1       0.87      0.65      0.74       649\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.83      0.79      0.80      1523\n",
      "weighted avg       0.82      0.81      0.80      1523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       874\n",
      "           1       0.79      0.66      0.72       649\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.77      0.77      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model: KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.83      0.78       874\n",
      "           1       0.72      0.60      0.65       649\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.71      0.72      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83       874\n",
      "           1       0.80      0.67      0.73       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.77      0.78      1523\n",
      "weighted avg       0.79      0.79      0.78      1523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model: Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.59      0.67       874\n",
      "           1       0.58      0.77      0.66       649\n",
      "\n",
      "    accuracy                           0.66      1523\n",
      "   macro avg       0.68      0.68      0.66      1523\n",
      "weighted avg       0.69      0.66      0.66      1523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model: Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       874\n",
      "           1       0.72      0.71      0.71       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.75      0.75      0.75      1523\n",
      "weighted avg       0.76      0.76      0.76      1523\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, train_df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data for models that require it\n",
    "scaler = StandardScaler()\n",
    "X_train_split_scaled = scaler.fit_transform(X_train_split)\n",
    "X_val_split_scaled = scaler.transform(X_val_split)\n",
    "\n",
    "models = {\n",
    "   'Logistic Regression' : LogisticRegression(max_iter=1000),\n",
    "   'SVM' : SVC(),\n",
    "   'Random Forest' : RandomForestClassifier(random_state=42),\n",
    "   'KNN' : KNeighborsClassifier(),\n",
    "   'Gradient Boosting' : GradientBoostingClassifier(),\n",
    "   'Naive Bayes' : GaussianNB(),\n",
    "   'Neural Network' : MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "   if name in ['Logistic Regression', 'SVM', 'Neural Network']:\n",
    "      model.fit(X_train_split_scaled, y_train_split)\n",
    "      y_pred = model.predict(X_val_split_scaled)\n",
    "   else:\n",
    "      model.fit(X_train_split, y_train_split)\n",
    "      y_pred = model.predict(X_val_split)\n",
    "   \n",
    "   print(f\"Model: {name}\")\n",
    "   print(classification_report(y_val_split, y_pred))\n",
    "   print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m    grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_split_scaled, y_train_split)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m    \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparameter_grid = {\n",
    "   ('Gradient Boosting' , models.get('Gradient Boosting')) : {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7], 'subsample': [0.8, 0.9, 1.0]},\n",
    "   ('Random Forest', models.get('Random Forest')) : {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]},\n",
    "   ('Neural Network', models.get('Neural Network')) : { 'hidden_layer_sizes': [(50,), (100,), (100, 50)], 'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam'], 'alpha': [0.0001, 0.001, 0.01], 'learning_rate': ['constant', 'adaptive']},\n",
    "   ('SVM', models.get('SVM')) : {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "}\n",
    "\n",
    "for grid in hyperparameter_grid.keys():\n",
    "   grid_search = GridSearchCV(grid[1], hyperparameter_grid[grid], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "   if grid[0] in ['Neural Network', 'SVM']:\n",
    "      grid_search.fit(X_train_split_scaled, y_train_split)\n",
    "   else:\n",
    "      grid_search.fit(X_train_split, y_train_split)\n",
    "   print(f\"Best Parameters for {grid[0]}:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
